import streamlit as st
from modules import ui, database, audio

def render(supabase, client, user_id, target_role, tier, xp, question_db):
    if tier == 'basic' and xp < 20:
        st.warning("ğŸ”’ éœ€å‡ç´šæˆ–ç´¯ç© 20 XP è§£é–æ­¤åŠŸèƒ½")
        return

    q_list = question_db.get(target_role, [])
    memories = database.get_memories_by_role(supabase, target_role)
    answered_qs = set()
    for m in memories:
        if "ã€é—œæ–¼" in m['content'] and "ã€‘ï¼š" in m['content']:
            answered_qs.add(m['content'].split("ã€é—œæ–¼")[1].split("ã€‘ï¼š")[0])

    if "edit_target" not in st.session_state: st.session_state.edit_target = None
    
    current_q = None
    if st.session_state.edit_target:
        current_q = st.session_state.edit_target
        st.info(f"âœï¸ æ­£åœ¨é‡æ–°éŒ„è£½ï¼š{current_q}")
    else:
        for q in q_list:
            if q not in answered_qs:
                current_q = q
                break
    
    if len(q_list) > 0:
        st.progress(len(answered_qs) / len(q_list), text=f"é€²åº¦ï¼š{len(answered_qs)} / {len(q_list)}")

    col_l, col_r = st.columns([1.5, 1], gap="medium")
    
    with col_l:
        st.markdown("### ğŸ™ï¸ é€²è¡Œä¸­ä»»å‹™")
        if current_q:
            ui.render_question_card(current_q, len(answered_qs)+1, len(q_list))
            
            audio_ans = st.audio_input("éŒ„éŸ³å›ç­”", key=f"mem_ans_{current_q}")
            if "trans_text" not in st.session_state: st.session_state.trans_text = ""
            
            if audio_ans:
                trans = client.audio.transcriptions.create(model="whisper-1", file=audio_ans)
                st.session_state.trans_text = trans.text
                
                st.text_area("æ–‡å­—ç¢ºèª", value=st.session_state.trans_text, key="mem_edit")
                
                c1, c2 = st.columns(2)
                with c1:
                    if st.button("ğŸ”Š è©¦è½ AI å”¸"):
                        ai_voice = audio.generate_speech(st.session_state.trans_text, tier)
                        st.audio(ai_voice, format="audio/mp3")
                with c2:
                    if st.button("ğŸ’¾ å­˜å…¥ä¸¦è¨“ç·´", type="primary"):
                        database.save_memory_fragment(supabase, target_role, current_q, st.session_state.trans_text)
                        audio_ans.seek(0)
                        audio.train_voice_sample(audio_ans.read())
                        st.success("å·²å„²å­˜")
                        st.session_state.edit_target = None
                        st.session_state.trans_text = ""
                        st.rerun()
            
            if st.button("â­ï¸ è·³é"):
                database.save_memory_fragment(supabase, target_role, current_q, "(å·²ç•¥é)")
                st.rerun()
        else:
            st.success("ğŸ‰ æ‰€æœ‰é¡Œç›®å·²å®Œæˆï¼")

    with col_r:
        st.markdown("### ğŸ“œ å›æ†¶å­˜æ‘º")
        with st.container(height=500):
            for mem in memories:
                if "ã€é—œæ–¼" in mem['content']:
                    try:
                        q = mem['content'].split("ã€é—œæ–¼")[1].split("ã€‘ï¼š")[0]
                        a = mem['content'].split("ã€‘ï¼š")[1]
                        ui.render_history_card(q, a)
                        if st.button("ğŸ”„ é‡éŒ„", key=f"mem_re_{mem['id']}"):
                            st.session_state.edit_target = q
                            st.rerun()
                    except: pass
