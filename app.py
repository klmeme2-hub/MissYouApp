import streamlit as st
import requests
from openai import OpenAI
from streamlit_mic_recorder import mic_recorder # å¼•å…¥éŒ„éŸ³å¥—ä»¶
import io

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="æƒ³å¿µ - èªéŸ³å°è©±ç‰ˆ", page_icon="ğŸ¤")
st.title("ğŸ¤ æƒ³å¿µ (Miss You)")
st.subheader("ç¬¬ä¸‰éšæ®µï¼šè£ä¸Šè€³æœµ")

# --- å´é‚Šæ¬„ï¼šæ ¸å¿ƒè¨­å®š ---
with st.sidebar:
    st.header("ğŸ”‘ é‡‘é‘°è¨­å®š")
    elevenlabs_key = st.text_input("ElevenLabs API Key", type="password")
    openai_key = st.text_input("OpenAI API Key", type="password")
    
    st.divider()
    st.header("ğŸ§  éˆé­‚è¨­å®š")
    default_prompt = """ä½ ç¾åœ¨æ‰®æ¼”æˆ‘çš„çˆ¶è¦ªã€‚
ä½ çš„åå­—å«å¼µå¿—æ˜ã€‚
å€‹æ€§ï¼šæº«æŸ”ã€æ²ˆç©©ï¼Œå¶çˆ¾æœƒè¬›å†·ç¬‘è©±ã€‚
èªªè©±ç¿’æ…£ï¼šå–œæ­¡ç”¨ã€Œå‚»å­©å­ã€ã€ã€Œå°å§ã€çµå°¾ã€‚
è«‹ç”¨æº«æš–ã€åƒçˆ¶è¦ªä¸€æ¨£çš„å£å»å›è¦†æˆ‘ã€‚"""
    system_prompt = st.text_area("è¨­å®šäººè¨­ï¼š", default_prompt, height=250)

    # è²éŸ³ ID è¨­å®š
    st.divider()
    voice_id_input = st.text_input("ElevenLabs Voice ID", "")

# --- åˆå§‹åŒ– OpenAI ---
client = None
if openai_key:
    client = OpenAI(api_key=openai_key)

# --- åˆå§‹åŒ–èŠå¤©ç´€éŒ„ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- é¡¯ç¤ºæ­·å²å°è©± ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- æ ¸å¿ƒé‚è¼¯å€ ---
# æˆ‘å€‘å®šç¾©ä¸€å€‹è™•ç†å°è©±çš„å‡½æ•¸ï¼Œç„¡è«–æ˜¯æ‰“å­—é‚„æ˜¯èªéŸ³éƒ½èµ°é€™è£¡
def process_conversation(user_text):
    # 1. é¡¯ç¤ºä¸¦å„²å­˜ä½¿ç”¨è€…çš„è©±
    with st.chat_message("user"):
        st.markdown(user_text)
    st.session_state.messages.append({"role": "user", "content": user_text})

    # 2. AI æ€è€ƒ & èªªè©±
    if client and elevenlabs_key and voice_id_input:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            try:
                # çµ„åˆå°è©±ç´€éŒ„
                messages_for_ai = [{"role": "system", "content": system_prompt}] + st.session_state.messages
                
                # å‘¼å« GPT
                response = client.chat.completions.create(
                    model="gpt-4o", 
                    messages=messages_for_ai
                )
                ai_text = response.choices[0].message.content
                message_placeholder.markdown(ai_text)
                st.session_state.messages.append({"role": "assistant", "content": ai_text})

                # å‘¼å« ElevenLabs TTS
                tts_url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id_input}"
                headers = {
                    "xi-api-key": elevenlabs_key,
                    "Content-Type": "application/json"
                }
                data = {
                    "text": ai_text,
                    "model_id": "eleven_multilingual_v2",
                    "voice_settings": {"stability": 0.5, "similarity_boost": 0.8}
                }
                tts_response = requests.post(tts_url, json=data, headers=headers)
                
                if tts_response.status_code == 200:
                    st.audio(tts_response.content, format="audio/mp3", autoplay=True)
                else:
                    st.error(f"è²éŸ³ç”Ÿæˆå¤±æ•—: {tts_response.text}")

            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        st.error("è«‹æª¢æŸ¥ Key æ˜¯å¦éƒ½å·²å¡«å¯«")

# --- è¼¸å…¥å€åŸŸ (èªéŸ³ + æ–‡å­—) ---
st.divider()
col1, col2 = st.columns([1, 4])

with col1:
    st.write("ğŸ™ï¸ æŒ‰ä¸‹èªªè©±ï¼š")
    # éŒ„éŸ³æŒ‰éˆ•
    audio = mic_recorder(
        start_prompt="é–‹å§‹éŒ„éŸ³",
        stop_prompt="çµæŸä¸¦ç™¼é€", 
        key='recorder'
    )

with col2:
    # å‚³çµ±æ–‡å­—è¼¸å…¥æ¡†
    text_input = st.chat_input("æˆ–ç”¨æ‰“å­—çš„...")

# --- è™•ç†è¼¸å…¥é‚è¼¯ ---

# æƒ…æ³ A: ä½¿ç”¨è€…ç”¨äº†éŒ„éŸ³
if audio:
    # ç‚ºäº†é¿å…é‡è¤‡ç™¼é€ï¼Œæˆ‘å€‘æª¢æŸ¥é€™å€‹éŸ³æª”æ˜¯å¦å‰›è¢«è™•ç†é
    if "last_audio_id" not in st.session_state:
        st.session_state.last_audio_id = None
    
    # åªæœ‰ç•¶é€™æ˜¯æ–°çš„éŒ„éŸ³æ™‚æ‰åŸ·è¡Œ
    if audio['id'] != st.session_state.last_audio_id:
        st.session_state.last_audio_id = audio['id']
        
        if client:
            with st.spinner("æ­£åœ¨è½ä½ èªªè©±..."):
                # å°‡éŒ„éŸ³è³‡æ–™è½‰ç‚º OpenAI Whisper èƒ½è®€çš„æ ¼å¼
                audio_bytes = audio['bytes']
                audio_file = io.BytesIO(audio_bytes)
                audio_file.name = "voice.mp3" # å½è£æˆæª”æ¡ˆ
                
                # å‘¼å« Whisper (èªéŸ³è½‰æ–‡å­—)
                try:
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file
                    )
                    user_voice_text = transcript.text
                    # åŸ·è¡Œå°è©±æµç¨‹
                    process_conversation(user_voice_text)
                    
                except Exception as e:
                    st.error(f"è½ä¸æ‡‚ä½ çš„è²éŸ³: {e}")

# æƒ…æ³ B: ä½¿ç”¨è€…ç”¨äº†æ‰“å­—
if text_input:
    process_conversation(text_input)