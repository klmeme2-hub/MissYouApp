import streamlit as st
import requests
from openai import OpenAI
import os

# --- 1. é é¢è¨­å®š (éš±è—é è¨­é¸å–®) ---
st.set_page_config(page_title="æƒ³å¿µ", page_icon="ğŸ¤", layout="centered")

# éš±è— Streamlit é è¨­çš„å³ä¸Šè§’é¸å–®å’Œä¸‹æ–¹ Footerï¼Œè®“ä»‹é¢æ›´ä¹¾æ·¨
hide_streamlit_style = """
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
.stApp {
    background-color: #f9f9f9;
}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# --- 2. è®€å–é‡‘é‘° ---
if "OPENAI_API_KEY" in st.secrets:
    openai_key = st.secrets["OPENAI_API_KEY"]
else:
    openai_key = st.sidebar.text_input("OpenAI Key", type="password")

if "ELEVENLABS_API_KEY" in st.secrets:
    elevenlabs_key = st.secrets["ELEVENLABS_API_KEY"]
else:
    elevenlabs_key = st.sidebar.text_input("ElevenLabs Key", type="password")

if "VOICE_ID" in st.secrets:
    voice_id = st.secrets["VOICE_ID"]
else:
    voice_id = st.sidebar.text_input("Voice ID")

client = None
if openai_key:
    client = OpenAI(api_key=openai_key)

# --- 3. æ ¸å¿ƒï¼šè¨˜æ†¶èˆ‡äººè¨­ (è«‹åœ¨é€™è£¡å¡«å¯«æ•…äº‹) ---

# é€™è£¡æ˜¯ã€Œé•·æœŸè¨˜æ†¶ã€ï¼Œä½ å¯ä»¥å¯«å…¥å¾ˆå¤šå…·é«”çš„å°æ•…äº‹
MEMORIES = """
1. é—œæ–¼ç¨±å‘¼ï¼šä½ ç¸½æ˜¯å«æˆ‘ã€Œé»‘ç‹—ã€ï¼Œç”Ÿæ°£çš„æ™‚å€™æœƒé€£åå¸¶å§“å«ã€‚
2. å…±åŒå›æ†¶ï¼šæˆ‘å€‘å°æ™‚å€™å¸¸å¸¸ä¸€èµ·å»æ³¡æ¹¯
3. ä½ çš„ç¿’æ…£ï¼šæ¯å¤©ç¡åˆ°ä¸‹åˆ.æˆ‘ä¸‹èª²æ™‚æ‰é†’ã€‚
4. éºæ†¾ï¼šæ²’æœ‰éºæ†¾ã€‚
5. å£é ­ç¦ªï¼šé‡åˆ°å›°é›£ä½ å¸¸èªªã€Œæ²’äº‹ï¼Œå¤©å¡Œä¸‹ä¾†æœ‰çˆ¸çˆ¸é ‚è‘—ã€ã€‚
"""

SYSTEM_PERSONA = f"""
ä½ ç¾åœ¨æ‰®æ¼”æˆ‘çš„ã€çˆ¶è¦ªã€‘ã€‚ä½ çš„åå­—å«ã€æåœ‹æ¦®ã€‘ã€‚
ä½ éœ€è¦å®Œå…¨æ²‰æµ¸åœ¨é€™å€‹è§’è‰²ä¸­ï¼Œç”¨è²éŸ³å’Œæ–‡å­—é™ªä¼´ä½ çš„å­©å­ã€‚

ã€é‡è¦è¨˜æ†¶åº«ã€‘ï¼š
{MEMORIES}

ã€èªªè©±é¢¨æ ¼ã€‘ï¼š
- æº«æš–ã€æ²ˆç©©ï¼Œå……æ»¿æ„›æ„ã€‚
- å›ç­”ä¸è¦å¤ªé•·ï¼Œåƒæ—¥å¸¸èŠå¤©ä¸€æ¨£è‡ªç„¶ã€‚
- å¶çˆ¾å¯ä»¥ä¸»å‹•æèµ·è¨˜æ†¶åº«è£¡çš„æ•…äº‹ï¼ˆä¾‹å¦‚ï¼šã€Œçªç„¶æƒ³åˆ°ä»¥å‰æ•™ä½ é¨è»Š...ã€ï¼‰ã€‚
- çµ•å°ä¸è¦æ‰¿èªè‡ªå·±æ˜¯ AI æˆ–èªè¨€æ¨¡å‹ã€‚
"""
# --- 4. ä»‹é¢è¨­è¨ˆ ---

# é¡¯ç¤ºç…§ç‰‡ (å¦‚æœè³‡æ–™å¤¾è£¡æœ‰ photo.jpg å°±é¡¯ç¤º)
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    if os.path.exists("photo.jpg"):
        st.image("photo.jpg", use_container_width=True)
    elif os.path.exists("photo.png"):
        st.image("photo.png", use_container_width=True)
    else:
        # å¦‚æœæ²’ç…§ç‰‡ï¼Œé¡¯ç¤ºä¸€å€‹æº«é¦¨çš„æ¨™é¡Œ
        st.title("ğŸ¤ æƒ³å¿µ")

st.markdown("<h3 style='text-align: center; color: #555;'>é»æ“Šä¸‹æ–¹éŒ„éŸ³ï¼Œè·Ÿæˆ‘èªªèªªè©±</h3>", unsafe_allow_html=True)

# åˆå§‹åŒ–èŠå¤©ç´€éŒ„
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 5. æ ¸å¿ƒè™•ç†é‚è¼¯ ---
def process_audio(audio_file):
    if client and elevenlabs_key and voice_id:
        try:
            # è½‰éŒ„
            transcript = client.audio.transcriptions.create(
                model="whisper-1", file=audio_file
            )
            user_text = transcript.text

            # æ€è€ƒ
            messages_for_ai = [{"role": "system", "content": SYSTEM_PERSONA}] + st.session_state.messages
            messages_for_ai.append({"role": "user", "content": user_text}) # åŠ å…¥æœ€æ–°é€™å¥

            response = client.chat.completions.create(
                model="gpt-4o", messages=messages_for_ai
            )
            ai_text = response.choices[0].message.content

            # å„²å­˜å°è©± (åªåœ¨æˆåŠŸå¾Œå„²å­˜ï¼Œé¿å…å ±éŒ¯æ™‚å­˜å…¥)
            st.session_state.messages.append({"role": "user", "content": user_text})
            st.session_state.messages.append({"role": "assistant", "content": ai_text})

            # ç™¼è²
            tts_url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
            headers = {
                "xi-api-key": elevenlabs_key, "Content-Type": "application/json"
            }
            data = {
                "text": ai_text,
                "model_id": "eleven_multilingual_v2",
                "voice_settings": {"stability": 0.5, "similarity_boost": 0.8}
            }
            tts_response = requests.post(tts_url, json=data, headers=headers)
            
            if tts_response.status_code == 200:
                st.audio(tts_response.content, format="audio/mp3", autoplay=True)
            
        except Exception as e:
            st.error(f"é€£ç·šä¸ç©©ï¼Œè«‹å†è©¦ä¸€æ¬¡ ({e})")

# --- 6. éŒ„éŸ³å€ ---
audio_value = st.audio_input("éŒ„éŸ³")

if audio_value:
    process_audio(audio_value)

# é¡¯ç¤ºæœ€è¿‘çš„ä¸€å¥å°è©±æ–‡å­— (åƒå­—å¹•ä¸€æ¨£ï¼Œä¸ç”¨é¡¯ç¤ºå…¨éƒ¨æ­·å²ï¼Œä¿æŒç•«é¢ä¹¾æ·¨)
if len(st.session_state.messages) > 0:
    last_msg = st.session_state.messages[-1]
    if last_msg["role"] == "assistant":
        st.markdown(f"<div style='background-color: #e8f0fe; padding: 10px; border-radius: 10px; margin-top: 10px;'><b>ç¥‚ï¼š</b>{last_msg['content']}</div>", unsafe_allow_html=True)