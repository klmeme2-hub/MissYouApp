import streamlit as st
import requests
from openai import OpenAI
from supabase import create_client, Client
import numpy as np
import os
import json
import io
from pydub import AudioSegment # å¼•å…¥éŸ³è¨Šè™•ç†åº«

# --- 1. é é¢èˆ‡ UI è¨­å®š ---
st.set_page_config(page_title="æƒ³å¿µ", page_icon="ğŸ¤", layout="centered")

custom_css = """
<style>
    .stApp, p, h1, h2, h3, label, div, span, button { color: #333333 !important; }
    div[data-baseweb="select"] > div { background-color: #FFFFFF !important; color: #333333 !important; }
    div[data-baseweb="popover"] li { background-color: #FFFFFF !important; color: #333333 !important; }
    div[data-baseweb="popover"] li:hover { background-color: #E3F2FD !important; }
    .ai-bubble {
        background-color: #FFFFFF;
        padding: 20px;
        border-radius: 15px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.05);
        border-left: 5px solid #4A90E2;
        margin: 10px 0;
        color: #333333;
    }
    .question-card {
        background-color: #E3F2FD;
        padding: 25px;
        border-radius: 12px;
        margin-bottom: 20px;
        border: 1px solid #BBDEFB;
        text-align: center;
    }
    .q-text {
        font-size: 20px;
        font-weight: bold;
        color: #1565C0 !important;
        margin-bottom: 10px;
    }
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# --- 2. åˆå§‹åŒ–èˆ‡è¨­å®š ---
if "SUPABASE_URL" not in st.secrets or "ADMIN_PASSWORD" not in st.secrets:
    st.error("âš ï¸ è«‹å…ˆè¨­å®š Secrets")
    st.stop()

openai_key = st.secrets["OPENAI_API_KEY"]
elevenlabs_key = st.secrets["ELEVENLABS_API_KEY"]
voice_id = st.secrets["VOICE_ID"]
supabase_url = st.secrets["SUPABASE_URL"]
supabase_key = st.secrets["SUPABASE_KEY"]
admin_password = st.secrets["ADMIN_PASSWORD"]

client = OpenAI(api_key=openai_key)

@st.cache_resource
def init_supabase():
    return create_client(supabase_url, supabase_key)

supabase = init_supabase()

ROLE_MAPPING = {
    "å¦»å­": "wife",
    "ä¸ˆå¤«": "husband",
    "å…’å­": "son",
    "å¥³å…’": "daughter",
    "æœ‹å‹": "friend",
    "å­«å­": "grandson",
    "å…¶ä»–": "others"
}

# --- 3. è®€å–å¤–éƒ¨é¡Œåº« ---
@st.cache_data
def load_questions_from_file():
    try:
        with open('questions.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

question_db = load_questions_from_file()

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

def get_embedding(text):
    text = text.replace("\n", " ")
    return client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding

def save_memory_fragment(role, text_content):
    embedding = get_embedding(text_content)
    data = {"role": role, "content": text_content, "embedding": embedding}
    supabase.table("memories").insert(data).execute()
    return True

def search_relevant_memories(role, query_text):
    try:
        query_vec = get_embedding(query_text)
        response = supabase.rpc(
            "match_memories",
            {"query_embedding": query_vec, "match_threshold": 0.5, "match_count": 3, "search_role": role}
        ).execute()
        return "\n".join([item['content'] for item in response.data])
    except: return ""

def save_persona_summary(role, content):
    data = {"role": role, "content": content}
    supabase.table("personas").upsert(data).execute()

def load_all_roles():
    try:
        res = supabase.table("personas").select("role").execute()
        return [i['role'] for i in res.data]
    except: return []

def load_persona(role):
    try:
        res = supabase.table("personas").select("content").eq("role", role).execute()
        return res.data[0]['content'] if res.data else None
    except: return None

# --- éŸ³è¨Šè™•ç†å‡½æ•¸ ---

def upload_nickname_audio(role, audio_bytes):
    try:
        safe_role = ROLE_MAPPING.get(role, "others")
        file_path = f"nickname_{safe_role}.mp3"
        supabase.storage.from_("audio_clips").upload(
            file_path, audio_bytes, file_options={"content-type": "audio/mpeg", "upsert": "true"}
        )
        return True
    except Exception as e:
        st.error(f"å„²å­˜éŸ³æª”å¤±æ•—: {e}")
        return False

def get_nickname_audio_bytes(role):
    try:
        safe_role = ROLE_MAPPING.get(role, "others")
        file_path = f"nickname_{safe_role}.mp3"
        response = supabase.storage.from_("audio_clips").download(file_path)
        return response
    except:
        return None

def train_voice_sample(audio_bytes):
    try:
        url = f"https://api.elevenlabs.io/v1/voices/{voice_id}/edit"
        headers = {"xi-api-key": elevenlabs_key}
        files = {'files': ('training_sample.mp3', audio_bytes, 'audio/mpeg')}
        data = {'name': 'My Digital Clone'} 
        response = requests.post(url, headers=headers, data=data, files=files)
        return response.status_code == 200
    except Exception as e:
        print(f"è¨“ç·´ä¸Šå‚³å¤±æ•—: {e}")
        return False

# --- é—œéµä¿®æ­£ï¼šéŸ³è¨Šåˆä½µé‚è¼¯ ---
def merge_audio_clips(intro_bytes, main_bytes):
    """
    ä½¿ç”¨ pydub å°‡å…©æ®µéŸ³è¨Šç„¡ç¸«æ¥åˆ
    intro_bytes: æš±ç¨±éŒ„éŸ³
    main_bytes: AI ç”Ÿæˆçš„å…§å®¹
    """
    try:
        # è®€å–éŸ³è¨Š
        intro = AudioSegment.from_file(io.BytesIO(intro_bytes), format="mp3")
        main = AudioSegment.from_file(io.BytesIO(main_bytes), format="mp3")
        
        # å»ºç«‹ä¸€æ®µ 200ms (0.2ç§’) çš„éœéŸ³
        silence = AudioSegment.silent(duration=200)
        
        # æ‹¼æ¥: æš±ç¨± + éœéŸ³ + AIèªéŸ³
        combined = intro + silence + main
        
        # è¼¸å‡ºç‚º Bytes
        buffer = io.BytesIO()
        combined.export(buffer, format="mp3")
        return buffer.getvalue()
    except Exception as e:
        st.error(f"éŸ³è¨Šåˆä½µå¤±æ•—: {e}")
        # å¦‚æœåˆä½µå¤±æ•—ï¼Œè‡³å°‘å›å‚³ä¸»è¦å…§å®¹ï¼Œä¸è¦è®“ç¨‹å¼å´©æ½°
        return main_bytes

# --- 5. æ¬Šé™ç®¡ç† ---
if "is_admin" not in st.session_state: st.session_state.is_admin = False

def check_pass():
    if st.session_state.pwd_input == admin_password:
        st.session_state.is_admin = True
        st.session_state.pwd_input = ""
    else: st.error("å¯†ç¢¼éŒ¯èª¤")

# --- 6. ä¸»ä»‹é¢ ---
st.title("ğŸ¤ æƒ³å¿µ")

if not st.session_state.is_admin:
    # === è¦ªå‹å‰å° ===
    roles = load_all_roles()
    if not roles:
        st.info("â˜ï¸ å°šæœªå»ºç«‹æ•¸ä½äººæ ¼")
    else:
        col_sel, col_pic = st.columns([2, 1])
        with col_sel:
            st.markdown("#### ğŸ‘‹ æ‚¨å¥½ï¼Œè«‹å•æ‚¨æ˜¯æˆ‘çš„...ï¼Ÿ")
            sel_role = st.selectbox("èº«åˆ†", roles, label_visibility="collapsed")
            persona_summary = load_persona(sel_role)
            if persona_summary: st.success(f"å·²é€£çµï¼š{sel_role}æ¨¡å¼")
        with col_pic:
            if os.path.exists("photo.jpg"): st.image("photo.jpg", use_container_width=True)

        if "chat_history" not in st.session_state: st.session_state.chat_history = []

        def process_chat(audio_file):
            try:
                transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_file)
                user_text = transcript.text
                if not user_text or len(user_text.strip()) < 2:
                    st.warning("ğŸ‘‚ è«‹å†èªªä¸€æ¬¡..."); return

                with st.spinner("æ€è€ƒèˆ‡æª¢ç´¢ä¸­..."):
                    relevant_memory = search_relevant_memories(sel_role, user_text)
                    
                    has_nickname_audio = get_nickname_audio_bytes(sel_role) is not None
                    
                    nickname_instruction = ""
                    if has_nickname_audio:
                        # å‘Šè¨´ AI ä¸è¦è‡ªå·±èªªå‡ºæš±ç¨±ï¼Œåªè¦èªªå¾Œé¢çš„è©±å°±å¥½
                        nickname_instruction = "ã€ç‰¹æ®ŠæŒ‡ä»¤ã€‘ï¼šä½ çš„å›æ‡‰**ä¸è¦**åŒ…å«å°æ–¹çš„æš±ç¨±æˆ–å•å€™èªï¼Œç›´æ¥è¬›å…§å®¹ã€‚å› ç‚ºç³»çµ±æœƒè‡ªå‹•åœ¨é–‹é ­æ’­æ”¾çœŸå¯¦çš„æš±ç¨±éŒ„éŸ³ã€‚"
                    else:
                        nickname_instruction = "è«‹åœ¨é–‹é ­è‡ªç„¶å‘¼å–šå°æ–¹çš„æš±ç¨±ã€‚"

                    system_instruction = f"""
                    {persona_summary}
                    ã€æ·±å±¤è¨˜æ†¶ã€‘ï¼š{relevant_memory}
                    {nickname_instruction}
                    èªæ°£è¦è‡ªç„¶ï¼ŒåŒ…å«å‘¼å¸æ„Ÿã€‚
                    """
                    
                    msgs = [{"role": "system", "content": system_instruction}] + st.session_state.chat_history[-6:]
                    msgs.append({"role": "user", "content": user_text})

                    res = client.chat.completions.create(model="gpt-4o-mini", messages=msgs)
                    ai_text = res.choices[0].message.content
                    
                    # é¡¯ç¤ºæ–‡å­—
                    st.session_state.chat_history.append({"role": "user", "content": user_text})
                    st.session_state.chat_history.append({"role": "assistant", "content": ai_text})

                    # --- éŸ³è¨Šç”Ÿæˆèˆ‡è™•ç† ---
                    final_audio_bytes = b""
                    ai_audio_bytes = b""

                    # 1. ç”Ÿæˆ AI éƒ¨åˆ†çš„èªéŸ³
                    if ai_text:
                        tts_url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
                        headers = {"xi-api-key": elevenlabs_key, "Content-Type": "application/json"}
                        data = {
                            "text": ai_text, 
                            "model_id": "eleven_multilingual_v2", 
                            "voice_settings": {"stability": 0.4, "similarity_boost": 0.65}
                        }
                        tts_res = requests.post(tts_url, json=data, headers=headers)
                        if tts_res.status_code == 200:
                            ai_audio_bytes = tts_res.content

                    # 2. é€²è¡Œæ‹¼æ¥ (å¦‚æœéœ€è¦)
                    if has_nickname_audio and ai_audio_bytes:
                        nickname_bytes = get_nickname_audio_bytes(sel_role)
                        if nickname_bytes:
                            # å‘¼å«åˆä½µå‡½æ•¸
                            final_audio_bytes = merge_audio_clips(nickname_bytes, ai_audio_bytes)
                        else:
                            final_audio_bytes = ai_audio_bytes
                    else:
                        final_audio_bytes = ai_audio_bytes

                    # 3. æ’­æ”¾
                    if final_audio_bytes:
                        st.audio(final_audio_bytes, format="audio/mp3", autoplay=True)

            except Exception as e: st.error(f"Error: {e}")

        st.divider()
        st.markdown("##### ğŸ™ï¸ æŒ‰ä¸‹éŒ„éŸ³è·Ÿæˆ‘èªªè©±ï¼š")
        val = st.audio_input("éŒ„éŸ³", key="rec_pub")
        if val and persona_summary: process_chat(val)
        if st.session_state.chat_history:
            last = st.session_state.chat_history[-1]
            if last["role"] == "assistant":
                st.markdown(f'<div class="ai-bubble"><b>ç¥‚èªªï¼š</b><br>{last["content"]}</div>', unsafe_allow_html=True)

    st.divider()
    with st.expander("ğŸ”’ æœƒå“¡ç™»å…¥"):
        st.text_input("å¯†ç¢¼", type="password", key="pwd_input", on_change=check_pass)

else:
    # === ç®¡ç†å“¡å¾Œå° ===
    st.success("ğŸ”“ ç®¡ç†å“¡æ¨¡å¼")
    if st.button("ç™»å‡º"):
        st.session_state.is_admin = False
        st.rerun()

    tab1, tab2, tab3 = st.tabs(["ğŸ“ åŸºç¤äººè¨­", "ğŸ§  å›æ†¶è£œå®Œ", "ğŸ¯ å®Œç¾æš±ç¨±"])

    with tab1:
        st.caption("è¨­å®šå°è©±èªæ°£èˆ‡åŸºç¤è³‡è¨Š")
        c1, c2 = st.columns(2)
        with c1: t_role = st.selectbox("å°è±¡", list(ROLE_MAPPING.keys()), key="tr")
        with c2: member_name = st.text_input("æ‚¨çš„åå­—", value="çˆ¸çˆ¸", key="mn")
        nickname = st.text_input("å°ˆå±¬æš±ç¨±", placeholder="ä¾‹å¦‚ï¼šå¯¶è²", key="nk")
        
        up_file = st.file_uploader(f"ä¸Šå‚³èˆ‡ã€{t_role}ã€‘çš„ç´€éŒ„", type="txt")
        if st.button("âœ¨ ç”ŸæˆåŸºç¤äººè¨­"):
            if up_file:
                with st.spinner("åˆ†æä¸­..."):
                    raw = up_file.read().decode("utf-8")
                    prompt = f"åˆ†æå°è©±ã€‚ä¸»è§’å°{t_role}çš„èªªè©±é¢¨æ ¼ã€‚å°ˆå±¬æš±ç¨±æ˜¯ã€Œ{nickname}ã€ã€‚è«‹ç”ŸæˆSystem Promptã€‚è³‡æ–™ï¼š{raw[-20000:]}"
                    res = client.chat.completions.create(model="gpt-4o", messages=[{"role": "user", "content": prompt}])
                    save_persona_summary(t_role, res.choices[0].message.content)
                    st.success("æ›´æ–°å®Œæˆ")

    with tab2:
        st.caption("å›æ†¶è£œå®Œè¨ˆç•«")
        q_role = st.selectbox("è£œå……å°è±¡å›æ†¶", list(question_db.keys()), key="q_role")
        q_list = question_db.get(q_role, ["(ç„¡é¡Œç›®)"])
        if "q_index" not in st.session_state: st.session_state.q_index = 0
        if "current_role_q" not in st.session_state: st.session_state.current_role_q = q_role
        if st.session_state.current_role_q != q_role:
            st.session_state.q_index = 0
            st.session_state.current_role_q = q_role

        if st.session_state.q_index < len(q_list):
            current_q = q_list[st.session_state.q_index]
            st.progress((st.session_state.q_index + 1) / len(q_list), text=f"é€²åº¦ï¼š{st.session_state.q_index + 1} / {len(q_list)}")
            st.markdown(f'<div class="question-card"><div class="q-text">Q{st.session_state.q_index + 1}: {current_q}</div></div>', unsafe_allow_html=True)
            
            audio_ans = st.audio_input("éŒ„éŸ³å›ç­”", key=f"ans_rec_{st.session_state.q_index}")
            c1, c2, c3 = st.columns([2, 1, 1])
            with c1:
                if st.button("âœ… æäº¤ä¸¦è¨“ç·´", type="primary", use_container_width=True):
                    if audio_ans:
                        with st.spinner("å­˜å…¥ä¸­..."):
                            trans = client.audio.transcriptions.create(model="whisper-1", file=audio_ans)
                            save_memory_fragment(q_role, f"ã€é—œæ–¼{current_q}ã€‘ï¼š{trans.text}")
                            audio_ans.seek(0)
                            train_voice_sample(audio_ans.read())
                            st.session_state.q_index += 1
                            st.rerun()
            with c2:
                if st.button("è·³é", use_container_width=True):
                    st.session_state.q_index += 1
                    st.rerun()
        else:
            st.success("å·²å®Œæˆæ‰€æœ‰é¡Œç›®ï¼")
            if st.button("é‡æ–°é–‹å§‹"):
                st.session_state.q_index = 0
                st.rerun()

    with tab3:
        st.subheader("ğŸ¯ å®Œç¾æš±ç¨±é‡ç¾")
        st.info("éŒ„è£½ä¸€æ®µçœŸå¯¦çš„å‘¼å–šï¼ŒAI æœƒåœ¨é–‹é ­ç›´æ¥æ’­æ”¾é€™æ®µéŒ„éŸ³ã€‚")
        nick_role = st.selectbox("éŒ„è£½çµ¦èª°è½ï¼Ÿ", list(ROLE_MAPPING.keys()), key="nick_role")
        st.markdown(f"è«‹æŒ‰ä¸‹éŒ„éŸ³ï¼Œå–Šä¸€è²çµ¦ã€{nick_role}ã€‘è½çš„æš±ç¨±ï¼š")
        real_nick_audio = st.audio_input("éŒ„è£½", key="real_nick_rec")
        if real_nick_audio:
            if st.button("ğŸ’¾ ä¸Šå‚³çœŸå¯¦è²éŸ³"):
                with st.spinner("è™•ç†ä¸­..."):
                    if upload_nickname_audio(nick_role, real_nick_audio.read()):
                        st.success("æˆåŠŸï¼")